# Day 11: Functions â€“ Creating an AI Data Processing Pipeline
# Functions keep code modular, reusable, and easier to maintain.

# 1. Defining a function to normalize input data
# In ML workflows, values are commonly scaled between 0 and 1
def normalize_pixel_values(pixels):
    normalized = []
    for p in pixels:
        normalized.append(round(p / 255.0, 3))
    return normalized
    

# 2. Defining a function to evaluate model deployment readiness
def check_deployment_ready(accuracy, threshold=0.90):
    if accuracy >= threshold:
        return True
    return False

# --- Applying the Functions ---
raw_data = [255, 128, 0, 45, 200]
processed_data = normalize_pixel_values(raw_data)

acc = 0.94
is_ready = check_deployment_ready(acc)

print(f"Normalized Tensors: {processed_data}")
print(f"Deployment Ready? {'Yes' if is_ready else 'No'}")
